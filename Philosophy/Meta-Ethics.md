- Clarification point: I tend to use "moral" to mean "normative". Some people might say there exist non-moral norms: statements to the effect "you should believe X when confronted with evidence Y" that are normative "shoulds" but not "moral" per se. I think the distinction they draw between moral and non-moral norms fuzzy and not useful, so I prefer to just say that if they think such a norm holds non-instrumentally, then they just have morals that value following that particular epistemic rule.
- Quite confident in moral anti-realism. I don't think there are any normative demands that are just out there in reality, true in and of themselves.
	- One concern here is that I'm so deep into this belief that I've sort of lost track of how to coherently imagine the concept of realist ethics. This could be a way I go wrong.
		- What does it mean for something to be "right", in a way that's just written into the universe? Like say there exists some higher epiphenomenal Platonic realm where everything is sorted into good and bad or assigned a score or whatnot - what makes this have normative force rather than just being arbitrary and not mattering?
		- One potential way to conceptualize realist ethics is if there's some formulation of ethics that is persuasive to all possible minds, or maybe all possible minds once at some sufficient threshold of "sanity", defined in terms of some kind of [[Idealization]] process.
			- Maybe if we get a "correct" answer to [[Idealization]], and it looks like it causes all minds to converge in some kind of way, then whatever ethical framework is universally appealing to minds that have converged in that way is "correct"?
				- Still unclear how exactly this gets normative force in the strict realist sense, but it does seem very close to the "any sufficiently intelligent (idealized) being will figure out and come to agree to X morality" formulation of realism.
	- Is this a tautology, given my [[Meta-Epistemology]]? Not necessarily. My definition of truth as "the belief that best serves my goals" does kind of take anti-realism as an assumption in the language, but I don't think there is actually a problem there. If you replace "my goals" with "the true correct goals", then every ethical system (except some weird ones) justifies itself as true. This leads us to moral pluralism, which is in practice the same as anti-realism. The only difference might be that it rules out ethical systems such as "the only thing that matters is not believing this statement to be true", which I didn't give much credence to begin with.
- Also pretty confident in error theory, even if realism is true. I don't think you can deduce ethical claims from empirical observations.
	- If you're a strict moral realist, but you don't think that ethical values are observable through physical experiments, and you believe in [[Physicalism]] when it comes to the human mind, then you have to admit that the truth-value of ethical propositions can never affect the state of a human mind.
		- This seems not quite airtight - I could make a similar argument about math that kind of sounds plausible. After all, the abstract entity of Math Itself never causally affects the human mind... feels like there's something complicated to deal with here.
			- Well, Math as an entity doesn't have causal effects, but there are experiments that will turn out differently if there is one of something vs two of something, or if certain mathematical propositions hold, or something like that. There are no experiments (as far as I know) that depend on the value of ethical propositions.
				- Unless you think that human moral judgments are an experiment that depend on the True Realist Universal Morality. But if you believe in [[Physicalism]], you don't believe that. So original argument holds.
- Even given both these things, the core of my meta-ethics is basically a Pascal's Wager that says there's a tiny chance that I'm wrong about the above, and there is some True Realist Universal Morality. Since it doesn't matter what I do in the case that I'm correct about the anti-realism, my decisions are entirely driven by the chance of TRUM.
	- As rough heuristics, do stuff that seems like it could plausibly be True Universal Morality, weighted more heavily as it gets more plausible.
		- Not exactly expected value, because doing probabilities and expected value calculations is really messy and complicated here.
	- There's a lot of complicated [[Moral Uncertainty]] stuff to deal with here because of the Wager structure. Very much unresolved.
	- Another potential problem with this is that someone could posit the existence of Super-Ethics, a form of normativity that overrides ethics, and that I would then be obliged to consider the plausibility of various Super-Ethical frameworks, and Super-Super-Ethics, etc.